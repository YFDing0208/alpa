{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "from jax import grad, jit, vmap\n",
    "from jax.nn import relu, one_hot\n",
    "from jax.random import PRNGKey, normal, randint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dyf/anaconda3/envs/alpa/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-12-25 08:36:49,464\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import alpa\n",
    "from alpa.model.model_util import TrainState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(key, layer_sizes):\n",
    "    keys = jax.random.split(key, len(layer_sizes))\n",
    "    return [normal(k, (m, n)) for m, n, k in zip(layer_sizes[:-1], layer_sizes[1:], keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim: int, hidden_dim: int, output_dim: int):\n",
    "    \"\"\"创建一个简单的由4层全连接层组成的模型\"\"\"\n",
    "    def init_layer(key, input_dim, output_dim):\n",
    "        w_key, b_key = jax.random.split(key)\n",
    "        w = jax.random.normal(w_key, (input_dim, output_dim)) * jnp.sqrt(1 / input_dim)\n",
    "        b = jnp.zeros(output_dim)\n",
    "        return w, b\n",
    "    \n",
    "    key = jax.random.PRNGKey(0)\n",
    "    key1, key2, key3, key4 = jax.random.split(key, 4)\n",
    "\n",
    "    params = {\n",
    "        'fc1': init_layer(key1, input_dim, hidden_dim),\n",
    "        'fc2': init_layer(key2, hidden_dim, hidden_dim),\n",
    "        'fc3': init_layer(key3, hidden_dim, hidden_dim),\n",
    "        'fc4': init_layer(key4, hidden_dim, output_dim),\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(params, x):\n",
    "    *hidden, out = params\n",
    "    for layer in hidden:\n",
    "        x = relu(jnp.dot(x, layer))\n",
    "    return jnp.dot(x, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(params, x):\n",
    "    x = jnp.dot(x, params['fc1'][0]) + params['fc1'][1]\n",
    "    x = jax.nn.relu(x)\n",
    "    x = jnp.dot(x, params['fc2'][0]) + params['fc2'][1]\n",
    "    x = jax.nn.relu(x)\n",
    "    x = jnp.dot(x, params['fc3'][0]) + params['fc3'][1]\n",
    "    x = jax.nn.relu(x)\n",
    "    x = jnp.dot(x, params['fc4'][0]) + params['fc4'][1]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(params, x, y):\n",
    "    logits = forward_pass(params, x)\n",
    "    loss = optax.softmax_cross_entropy(logits, y).mean()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(rng, input_dim, hidden_dim, output_dim, learning_rate):\n",
    "    params = create_model(input_dim, hidden_dim, output_dim)\n",
    "    tx = optax.adamw(learning_rate=learning_rate)\n",
    "    return TrainState.create(apply_fn=forward_pass, params=params, tx=tx, dynamic_scale=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义训练步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(state, batch):\n",
    "    def loss_fn_inner(params):\n",
    "        x, y = batch\n",
    "        return loss_fn(params, x, y)\n",
    "    \n",
    "    grad_fn = alpa.value_and_grad(loss_fn_inner)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 设置随机种子\n",
    "rng = PRNGKey(0)\n",
    "\n",
    "input_dim = 784\n",
    "hidden_dim = 128\n",
    "output_dim = 10\n",
    "\n",
    "num_samples = 1000\n",
    "x_train = normal(rng, (num_samples, input_dim))\n",
    "y_train = randint(rng, (num_samples, ), 0, output_dim, dtype=int)\n",
    "y_train = one_hot(y_train, output_dim)\n",
    "\n",
    "\n",
    "num_train_epochs = 1\n",
    "train_batch_size = 32\n",
    "num_batches = num_samples // train_batch_size\n",
    "micro_batch_size = 8\n",
    "learning_rate = 5e-5\n",
    "\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "\n",
    "state = create_train_state(init_rng, input_dim, hidden_dim, output_dim, learning_rate,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jax训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from flax.training.common_utils import shard\n",
    "from jax.experimental import maps, pjit\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TrainState' object has no attribute 'replicate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create parallel version of the train step\u001b[39;00m\n\u001b[1;32m      2\u001b[0m devices \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mdevices()\n\u001b[0;32m----> 3\u001b[0m state \u001b[38;5;241m=\u001b[39m shard(\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplicate\u001b[49m(), devices)\n\u001b[1;32m      4\u001b[0m train_step_p \u001b[38;5;241m=\u001b[39m pjit(\n\u001b[1;32m      5\u001b[0m     train_step,\n\u001b[1;32m      6\u001b[0m     in_shardings\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, P(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[1;32m      7\u001b[0m     out_shardings\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m      8\u001b[0m     donate_argnums\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,),\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     12\u001b[0m train_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TrainState' object has no attribute 'replicate'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Create parallel version of the train step\n",
    "devices = jax.devices()\n",
    "state = shard(state.replicate(), devices)\n",
    "train_step_p = pjit(\n",
    "    train_step,\n",
    "    in_shardings=(None, P('batch')),\n",
    "    out_shardings=(None, None),\n",
    "    donate_argnums=(0,),\n",
    ")\n",
    "\n",
    "\n",
    "train_time = 0\n",
    "last_time = time.time()\n",
    "epochs = tqdm(range(num_train_epochs), desc=f\"Epoch ... (1/{num_train_epochs})\", position=0)\n",
    "\n",
    "for epoch in epochs:\n",
    "    # ======================== Training ================================\n",
    "    train_start = time.time()\n",
    "\n",
    "    # Create sampling rng\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "    train_metrics = []\n",
    "\n",
    "    train_step_progress_bar = tqdm(total=num_batches, desc=\"Training...\", position=1, leave=False)\n",
    "    # train\n",
    "    for step in range(num_batches):\n",
    "        start_idx = step * train_batch_size\n",
    "        end_idx = (step + 1) * train_batch_size\n",
    "        batch = (x_train[start_idx:end_idx], y_train[start_idx:end_idx])\n",
    "        state, loss = train_step_p(state, batch)\n",
    "        train_metrics.append(loss)\n",
    "\n",
    "        train_step_progress_bar.update(1)\n",
    "\n",
    "    latency = time.time() - last_time\n",
    "    images_per_second = num_samples / latency\n",
    "    train_time += time.time() - train_start\n",
    "    last_time = time.time()\n",
    "\n",
    "    train_step_progress_bar.close()\n",
    "    epochs.write(\n",
    "        f\"Epoch... ({epoch + 1}/{num_train_epochs} | Loss: {jnp.mean(train_metrics)}, \"\n",
    "        f\"Throughput: {images_per_second:.2f} images/s\"\n",
    "    )\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## alpa_heter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-25 08:37:38,253\tINFO worker.py:1636 -- Connecting to existing Ray cluster at address: 172.17.0.6:6379...\n",
      "2024-12-25 08:37:38,312\tINFO worker.py:1821 -- Connected to Ray cluster.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ray with address auto\n",
      "GPUlets enabled, getting GPU info from node: 172.17.0.6\n",
      "\n",
      "all_host_info: [{'NodeID': 'efb40bc566dcb61bd44a8056a21b0412ed15a6a8f44ad1256c05cb72', 'Alive': True, 'NodeManagerAddress': '172.17.0.6', 'NodeManagerHostname': '7b1e0a1f1e75', 'NodeManagerPort': 37155, 'ObjectManagerPort': 44293, 'ObjectStoreSocketName': '/tmp/ray/session_2024-12-25_08-24-58_226897_50998/sockets/plasma_store', 'RayletSocketName': '/tmp/ray/session_2024-12-25_08-24-58_226897_50998/sockets/raylet', 'MetricsExportPort': 53761, 'NodeName': '172.17.0.6', 'RuntimeEnvAgentPort': 62597, 'DeathReason': 0, 'DeathReasonMessage': '', 'alive': True, 'Resources': {'node:__internal_head__': 1.0, 'node:172.17.0.6': 1.0, 'GPU': 2.0, 'accelerator_type:G': 1.0, 'CPU': 40.0, 'object_store_memory': 39348381696.0, 'memory': 81812890624.0}, 'Labels': {'ray.io/node_id': 'efb40bc566dcb61bd44a8056a21b0412ed15a6a8f44ad1256c05cb72'}}]\n",
      "all_host_num_devices: [2]\n"
     ]
    }
   ],
   "source": [
    "alpa.init(cluster=\"ray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_method = alpa.parallel_method.PipeshardParallel(stage_option=\"auto\")\n",
    "p_train_step = alpa.parallelize(train_step,\n",
    "                                method=train_method)\n",
    "                                #donate_argnums=(0,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:alpa.pipeline_parallel.layer_construction:Too few non-trivial ops (dot, conv), which may influence auto-sharding performance\n",
      "INFO:alpa.pipeline_parallel.stage_construction:num_devices = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-*-*-*-*-VirtualPhysicalMesh-*-*-*-*-\n",
      "host_ids:  [0]\n",
      "host_info:  [{'NodeID': 'efb40bc566dcb61bd44a8056a21b0412ed15a6a8f44ad1256c05cb72', 'Alive': True, 'NodeManagerAddress': '172.17.0.6', 'NodeManagerHostname': '7b1e0a1f1e75', 'NodeManagerPort': 37155, 'ObjectManagerPort': 44293, 'ObjectStoreSocketName': '/tmp/ray/session_2024-12-25_08-24-58_226897_50998/sockets/plasma_store', 'RayletSocketName': '/tmp/ray/session_2024-12-25_08-24-58_226897_50998/sockets/raylet', 'MetricsExportPort': 53761, 'NodeName': '172.17.0.6', 'RuntimeEnvAgentPort': 62597, 'DeathReason': 0, 'DeathReasonMessage': '', 'alive': True, 'Resources': {'node:__internal_head__': 1.0, 'node:172.17.0.6': 1.0, 'GPU': 2.0, 'accelerator_type:G': 1.0, 'CPU': 40.0, 'object_store_memory': 39348381696.0, 'memory': 81812890624.0}, 'Labels': {'ray.io/node_id': 'efb40bc566dcb61bd44a8056a21b0412ed15a6a8f44ad1256c05cb72'}}]\n",
      "num_devices_per_host:  2\n",
      "devices:  [[0, 1]]\n",
      "num_gpus:  2\n",
      "node efb40bc566dcb61bd44a8056a21b0412ed15a6a8f44ad1256c05cb72 has 2 GPUs\n",
      "Physical GPU: NVIDIA GeForce RTX 3090\n",
      "    GPUID: 0\n",
      "    UUID: GPU-174678e7-9442-e990-ec4a-2208b906ffc6\n",
      "    Memory: 24.0 GB\n",
      "    Node ID: efb40bc566dcb61bd44a8056a21b0412ed15a6a8f44ad1256c05cb72\n",
      "    Compute Capacity: 35.6\n",
      "    Number of GPUlets: 1\n",
      "    GPUlets:\n",
      "        GPUlets ID: 0, Memory: 24.0 GB\n",
      "        belongs to NVIDIA GeForce RTX 3090: GPU-174678e7-9442-e990-ec4a-2208b906ffc6\n",
      "Physical GPU: NVIDIA GeForce RTX 3090\n",
      "    GPUID: 1\n",
      "    UUID: GPU-b5ef7776-5b2c-5b84-3aa9-c0b1a604e7f0\n",
      "    Memory: 24.0 GB\n",
      "    Node ID: efb40bc566dcb61bd44a8056a21b0412ed15a6a8f44ad1256c05cb72\n",
      "    Compute Capacity: 35.6\n",
      "    Number of GPUlets: 1\n",
      "    GPUlets:\n",
      "        GPUlets ID: 1, Memory: 24.0 GB\n",
      "        belongs to NVIDIA GeForce RTX 3090: GPU-b5ef7776-5b2c-5b84-3aa9-c0b1a604e7f0\n",
      "-*-*-*-*-VirtualPhysicalMesh  END-*-*-*-*-\n",
      "stage_option.submesh_physical_shape_space :power_of_two\n",
      "stage_option.submesh_logical_shape_space: single_node_model_parallel\n",
      "stage_option.manually_specified_submeshes: None\n",
      "i = 0, self.num_gpulets = 2\n",
      "virtual_mesh from gpulet_mesh\n",
      "\n",
      "-*-*-*-*-VirtualPhysicalMesh-*-*-*-*-\n",
      "host_ids:  [0]\n",
      "host_info:  [{'NodeID': 'efb40bc566dcb61bd44a8056a21b0412ed15a6a8f44ad1256c05cb72', 'Alive': True, 'NodeManagerAddress': '172.17.0.6', 'NodeManagerHostname': '7b1e0a1f1e75', 'NodeManagerPort': 37155, 'ObjectManagerPort': 44293, 'ObjectStoreSocketName': '/tmp/ray/session_2024-12-25_08-24-58_226897_50998/sockets/plasma_store', 'RayletSocketName': '/tmp/ray/session_2024-12-25_08-24-58_226897_50998/sockets/raylet', 'MetricsExportPort': 53761, 'NodeName': '172.17.0.6', 'RuntimeEnvAgentPort': 62597, 'DeathReason': 0, 'DeathReasonMessage': '', 'alive': True, 'Resources': {'node:__internal_head__': 1.0, 'node:172.17.0.6': 1.0, 'GPU': 2.0, 'accelerator_type:G': 1.0, 'CPU': 40.0, 'object_store_memory': 39348381696.0, 'memory': 81812890624.0}, 'Labels': {'ray.io/node_id': 'efb40bc566dcb61bd44a8056a21b0412ed15a6a8f44ad1256c05cb72'}}]\n",
      "num_devices_per_host:  1\n",
      "devices:  [[0]]\n",
      "num_gpus:  1\n",
      "node efb40bc566dcb61bd44a8056a21b0412ed15a6a8f44ad1256c05cb72 has 1 GPUs\n",
      "Physical GPU: NVIDIA GeForce RTX 3090\n",
      "    GPUID: 0\n",
      "    UUID: GPU-174678e7-9442-e990-ec4a-2208b906ffc6\n",
      "    Memory: 24.0 GB\n",
      "    Node ID: efb40bc566dcb61bd44a8056a21b0412ed15a6a8f44ad1256c05cb72\n",
      "    Compute Capacity: 35.6\n",
      "    Number of GPUlets: 1\n",
      "    GPUlets:\n",
      "        GPUlets ID: 0, Memory: 24.0 GB\n",
      "        belongs to NVIDIA GeForce RTX 3090: GPU-174678e7-9442-e990-ec4a-2208b906ffc6\n",
      "-*-*-*-*-VirtualPhysicalMesh  END-*-*-*-*-\n",
      "-------------------- Automatic stage clustering --------------------\n",
      "gpulet_submesh_choices: ((1, 1),)\n",
      "- Profiling for submesh 0 (1, 1):\n",
      "gpulet_indices:  (0,)\n",
      "i = 0, self.num_gpulets = 2\n",
      "gpulet_indices:  (1,)\n",
      "i = 1, self.num_gpulets = 2\n",
      "- Generate all stage infos (Jaxpr -> HLO)\n",
      "num_layers: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile_results before check: {}\n",
      "get_compute_cost() in stage_profiling.py:\n",
      "==auto_sharding_configs:\n",
      "config: (<alpa.shard_parallel.auto_sharding.LogicalDeviceMesh object at 0x7f4b25f5d370>, {'force_batch_dim_to_mesh_dim': 0})\n",
      "config: (<alpa.shard_parallel.auto_sharding.LogicalDeviceMesh object at 0x7f4b25f5d9a0>, {})\n",
      "==stages to profile:\n",
      "stage_idx: (0, 0, 0, 0)\n",
      "stage_idx: (0, 0, 0, 1)\n",
      "stage_idx: (0, 1, 0, 0)\n",
      "stage_idx: (0, 1, 0, 1)\n",
      "stage_idx: (1, 1, 0, 0)\n",
      "stage_idx: (1, 1, 0, 1)\n",
      "==sliced_virtual_meshes:\n",
      "sliced_virtual_mesh 0: host_ids->[0]                         num_devices_per_host->1  devices->[[0]]\n",
      "sliced_virtual_mesh 1: host_ids->[0]                         num_devices_per_host->1  devices->[[1]]\n",
      "- Compile all stages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Profile all stages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1/12 [00:05<00:59,  5.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ProfileWorker pid=51994)\u001b[0m INFO:alpa.device_mesh:num_devices_per_host: 1\n",
      "\u001b[36m(ProfileWorker pid=51994)\u001b[0m INFO:alpa.device_mesh:num_gpus: 1\n",
      "\u001b[36m(ProfileWorker pid=51994)\u001b[0m INFO:alpa.device_mesh:Launching workers on hosts: [0]\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m INFO:alpa.mesh_executable:num_devices: 1, len(worker.backend.devices()): 1\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m 2024-12-25 08:38:01.252178: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:219] failed to create cublas handle: cublas error\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m 2024-12-25 08:38:01.252230: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:221] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m 2024-12-25 08:38:01.252308: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2156] Execution of replica 0 failed: INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m 2024-12-25 08:38:01.309422: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:219] failed to create cublas handle: cublas error\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m 2024-12-25 08:38:01.309470: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:221] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m 2024-12-25 08:38:01.309508: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2156] Execution of replica 0 failed: INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m 2024-12-25 08:38:01.473913: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:219] failed to create cublas handle: cublas error\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m 2024-12-25 08:38:01.473954: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:221] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m 2024-12-25 08:38:01.473983: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2156] Execution of replica 0 failed: INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m 2024-12-25 08:38:01.739751: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:219] failed to create cublas handle: cublas error\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m 2024-12-25 08:38:01.739803: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:221] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m 2024-12-25 08:38:01.739845: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2156] Execution of replica 0 failed: INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m 2024-12-25 08:38:01.939957: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:219] failed to create cublas handle: cublas error\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m 2024-12-25 08:38:01.939998: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:221] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m 2024-12-25 08:38:01.940047: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2156] Execution of replica 0 failed: INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m 2024-12-25 08:38:02.029752: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:219] failed to create cublas handle: cublas error\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m 2024-12-25 08:38:02.029801: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:221] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m 2024-12-25 08:38:02.029836: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2156] Execution of replica 0 failed: INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m 2024-12-25 08:38:02.261331: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:219] failed to create cublas handle: cublas error\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m 2024-12-25 08:38:02.261374: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:221] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m 2024-12-25 08:38:02.261405: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2156] Execution of replica 0 failed: INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n",
      "\u001b[36m(ProfileWorker pid=51996)\u001b[0m INFO:alpa.device_mesh:num_devices_per_host: 1\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m 2024-12-25 08:38:02.425049: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:219] failed to create cublas handle: cublas error\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m 2024-12-25 08:38:02.425090: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:221] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.\n",
      "\u001b[36m(MeshHostWorker pid=54495)\u001b[0m 2024-12-25 08:38:02.425123: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2156] Execution of replica 0 failed: INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result[(0, 0, 0, 0), 0] = ModuleProfileResult(compute_cost=inf, peak_memory=0.001 GB, invar_size=0.001 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=1.869 GB)\n",
      "result[(0, 0, 0, 1), 0] = ModuleProfileResult(compute_cost=inf, peak_memory=0.001 GB, invar_size=0.001 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=1.869 GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 3/12 [00:05<00:13,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result[(0, 0, 0, 1), 1] = ModuleProfileResult(compute_cost=inf, peak_memory=0.001 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=1.869 GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 6/12 [00:06<00:03,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result[(0, 1, 0, 0), 0] = ModuleProfileResult(compute_cost=inf, peak_memory=0.001 GB, invar_size=0.001 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=1.869 GB)\n",
      "result[(0, 0, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.001 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=20.767 GB)\n",
      "result[(0, 1, 0, 0), 1] = ModuleProfileResult(compute_cost=inf, peak_memory=0.001 GB, invar_size=0.000 GB, outvar_size=0.001 GB, temp_buffer_size=0.000 GB, available_memory=1.869 GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 8/12 [00:06<00:01,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result[(0, 1, 0, 1), 1] = ModuleProfileResult(compute_cost=inf, peak_memory=0.001 GB, invar_size=0.000 GB, outvar_size=0.001 GB, temp_buffer_size=0.000 GB, available_memory=1.869 GB)\n",
      "result[(0, 1, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.001 GB, invar_size=0.001 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=20.767 GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 11/12 [00:06<00:00,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result[(1, 1, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.000 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=20.767 GB)\n",
      "result[(1, 1, 0, 0), 0] = ModuleProfileResult(compute_cost=inf, peak_memory=0.000 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=1.869 GB)\n",
      "result[(1, 1, 0, 1), 1] = ModuleProfileResult(compute_cost=inf, peak_memory=0.000 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=1.869 GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:06<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result[(1, 1, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.000 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=20.767 GB)\n",
      "Profiling for submesh 0 (1, 1) takes 9.12 seconds\n",
      "--------------------------------------------------\n",
      "Profile result saved to: profile-results-2024-12-25-08-38-02.npy\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: fffffffffffffffff0a612bae68291825611299701000000 Worker ID: b781bbdd396ece6446c185ac244083f45de663c363da738b7e330e1b Node ID: efb40bc566dcb61bd44a8056a21b0412ed15a6a8f44ad1256c05cb72 Worker IP address: 172.17.0.6 Worker port: 10043 Worker PID: 54495 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly by a signal. SystemExit is raised (sys.exit is called). Exit code: 1. The process receives a SIGTERM.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "no solution in auto stage construction.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m end_idx \u001b[38;5;241m=\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m train_batch_size\n\u001b[1;32m     13\u001b[0m batch \u001b[38;5;241m=\u001b[39m (x_train[start_idx:end_idx], y_train[start_idx:end_idx])\n\u001b[0;32m---> 14\u001b[0m state, loss \u001b[38;5;241m=\u001b[39m \u001b[43mp_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m train_metrics\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dump_debug_info_train_step:\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "File \u001b[0;32m/data/dyf/code/alpa/alpa/pipeline_parallel/compile_executable.py:117\u001b[0m, in \u001b[0;36mcompile_pipeshard_executable\u001b[0;34m(fun, in_tree, out_tree_thunk, static_argnums, donated_invars, batch_invars, virtual_mesh, num_microbatch, pipeline_schedule, default_as_option, layer_option, stage_option, global_input_shardings, stage_input_shardings, manual_shard_options, *avals)\u001b[0m\n\u001b[1;32m    112\u001b[0m     parsed_ms_option \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m#print(f\"closed_jaxpr: {closed_jaxpr}\")\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m#print(f\"full_batch_closed_jaxpr: {full_batch_closed_jaxpr}\")\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m pipeshard_config \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_pipeshard_executable_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclosed_jaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_batch_closed_jaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmicro_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_invars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvirtual_mesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_microbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline_schedule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_as_option\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage_option\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_input_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage_input_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparsed_ms_option\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompile time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mcompile_start_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m compile_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m/data/dyf/code/alpa/alpa/pipeline_parallel/compile_executable.py:184\u001b[0m, in \u001b[0;36mcompile_pipeshard_executable_internal\u001b[0;34m(closed_jaxpr, full_batch_closed_jaxpr, micro_batch_size, donated_invars, batch_invars, virtual_mesh, num_microbatch, pipeline_schedule, default_as_option, stage_option, name_base, global_input_shardings, global_output_shardings, stage_input_shardings, parsed_manual_sharding_option)\u001b[0m\n\u001b[1;32m    176\u001b[0m (jax_apply_layers,\n\u001b[1;32m    177\u001b[0m  apply_grad_global_info) \u001b[38;5;241m=\u001b[39m slice_apply_grad_for_stage_construction(\n\u001b[1;32m    178\u001b[0m      jax_pipeline_layers, apply_grad_jaxpr, microbatch_bound, global_invars,\n\u001b[1;32m    179\u001b[0m      global_outvars, donated_invars, accumulator_mapping, gensym_func,\n\u001b[1;32m    180\u001b[0m      inference_mode)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Construct pipeline stages by merging layers\u001b[39;00m\n\u001b[1;32m    183\u001b[0m (jax_pipeline_stages, stage_to_mesh, sliced_virtual_meshes,\n\u001b[0;32m--> 184\u001b[0m  manual_stage_option) \u001b[38;5;241m=\u001b[39m \u001b[43mcluster_layers_and_slice_mesh\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m     \u001b[49m\u001b[43mjax_pipeline_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvirtual_mesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulator_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m     \u001b[49m\u001b[43macc_grad_invars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macc_grad_outvars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_microbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmicro_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m     \u001b[49m\u001b[43mjax_apply_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_grad_global_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline_schedule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdefault_as_option\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage_option\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m num_meshes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sliced_virtual_meshes)\n\u001b[1;32m    190\u001b[0m debug_compilation_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage construction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/dyf/code/alpa/alpa/pipeline_parallel/stage_construction.py:836\u001b[0m, in \u001b[0;36mcluster_layers_and_slice_mesh\u001b[0;34m(layers, virtual_mesh, accumulator_mapping, acc_grad_invars, acc_grad_outvars, num_micro_batches, batch_size, jax_apply_layers, apply_grad_global_info, pipeline_schedule, default_as_option, stage_option)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    831\u001b[0m     _, solution \u001b[38;5;241m=\u001b[39m training_dp(num_layers, virtual_mesh\u001b[38;5;241m.\u001b[39mnum_devices,\n\u001b[1;32m    832\u001b[0m                               num_micro_batches, submesh_choices,\n\u001b[1;32m    833\u001b[0m                               num_autosharding_configs, compute_cost,\n\u001b[1;32m    834\u001b[0m                               max_n_succ_stages)\n\u001b[0;32m--> 836\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m solution \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno solution in auto stage construction.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;66;03m# Parse solution\u001b[39;00m\n\u001b[1;32m    839\u001b[0m forward_stage_layer_ids \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(start_id, end_id))\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (start_id, end_id), _, _ \u001b[38;5;129;01min\u001b[39;00m solution\n\u001b[1;32m    842\u001b[0m ]\n",
      "\u001b[0;31mAssertionError\u001b[0m: no solution in auto stage construction."
     ]
    }
   ],
   "source": [
    "dump_debug_info_train_step = True\n",
    "for epoch in range(num_train_epochs):\n",
    "    # ======================== Training ================================\n",
    "\n",
    "    # Create sampling rng\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "    train_metrics = []\n",
    "\n",
    "    # train\n",
    "    for step in range(num_batches):\n",
    "        start_idx = step * train_batch_size\n",
    "        end_idx = (step + 1) * train_batch_size\n",
    "        batch = (x_train[start_idx:end_idx], y_train[start_idx:end_idx])\n",
    "        state, loss = p_train_step(state, batch)\n",
    "        train_metrics.append(loss)\n",
    "\n",
    "        if dump_debug_info_train_step:\n",
    "            dump_debug_info_train_step = False\n",
    "            executable = p_train_step.get_last_executable()\n",
    "            executable.sync()\n",
    "            executable.dump_debug_info(\"alpa_debug_info\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from flax.training.common_utils import shard\n",
    "from jax.experimental import maps, pjit\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from flax.training.common_utils import shard\n",
    "from jax.experimental import maps, pjit\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from flax.training.common_utils import shard\n",
    "from jax.experimental import maps, pjit\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from flax.training.common_utils import shard\n",
    "from jax.experimental import maps, pjit\n",
    "\n",
    "from tqdm import tqdm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
